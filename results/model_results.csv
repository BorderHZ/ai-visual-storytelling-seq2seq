num_samples,duration,num_epochs,loss,val_loss,num_layers,cell_type,activation,hidden_dimension,learning_rate,gradient_clipping_value,optimizer,loss_history_filename,model_filename,reverse_sequence,notes
200775,47:24:15h,150,0.195949491072,7.784921676,2,gru,tanh,1024,0.0001,5.0,adam,./loss_logs/2018-01-29_00:37:26.csv,./trained_models/2018-01-29_00:37:26-2018-01-31_00:01:42.h5,True, all samples are shuflled on every epoch
200775,30:00:00h,100,0.2139,None,2,gru,tanh,1024,0.0001,5.0,adam,./loss_logs/2018-02-01_16:22:39.csv,./trained_models/2018-02-01_16:22:39-2018-02-03_00:22:15.h5,False, Added reccurence dropout of 0.2
200775,16:15:28h,50,2.00876955564,-1,2,gru,tanh,1024,0.0001,5.0,adam,./loss_logs/2018-02-03_14:00:00.csv,./trained_models/2018-02-03_14:00:00-2018-02-04_06:15:29.h5,False, Input dropout of 0.5
200775,35:23:15h,110,1.47597208434,-1,2,gru,tanh,1024,0.0001,5.0,adam,./loss_logs/2018-02-04_10:27:57.csv,./trained_models/2018-02-04_10:27:57-2018-02-05_21:51:13.h5,False, Input dropout of 0.5
200775,13:00:00h,50,1.844,-1,2,gru,tanh,1024,0.0001,5.0,adam,./loss_logs/2018-02-07_13:55:01.csv,./trained_models/2018-02-07_13:55:01-2018-02-08_03:15:08,False, dropout: input image and before softmax of 0.5, sending last 3 images. Very good results
200775,19:22:45h,50,0.823522208953,-1,2,gru,tanh,1024,0.0001,5.0,adam,./loss_logs/2018-02-08_16:58:54.csv,./trained_models/2018-02-08_16:58:54-2018-02-09_12:21:40.h5,False, Sending the previous sentence with one additional encoder, last_k =3. Overfit for 50 epochs
200775,9:34:01h,25,1.70632990266,-1,2,gru,tanh,1024,0.0001,5.0,adam,./loss_logs/2018-02-09_15:30:08.csv,./trained_models/2018-02-09_15:30:08-2018-02-10_01:04:10.h5,False, Sending the previous sentence with one additional encoder, last_k =3.
200775,12:52:40h,50,1.86891705098,-1,2,gru,tanh,1024,0.0001,5.0,adam,./loss_logs/2018-02-12_15:36:20.csv,./trained_models/2018-02-12_15:36:20-2018-02-13_04:29:01.h5,False,dropout: input image and before softmax of 0.5, sending last 2 images. Very good results
